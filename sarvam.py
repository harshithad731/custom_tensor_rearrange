# -*- coding: utf-8 -*-
"""Sarvam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CSNz0u9_oUGKsp2K3KLCpDu7Btl8VDS2

# rearrange: Custom Tensor Rearrangement Utility
Inspired by the elegance of einops.rearrange, this rearrange function is a compact, efficient, and readable numpy-native utility to reshape and permute multi-dimensional arrays using a declarative string pattern.

### Function Signature

rearrange(tensor: np.ndarray, pattern: str, **axes_lengths) -> np.ndarray
### Features
- Supports axis merging/splitting using (...) notation.

- Supports broadcasting dimensions (1 -> b).

- Handles ellipsis (...) for unknown axis positions.

- Works with batch dimensions and large-scale data.

- Numpy-only, no external dependencies.

- Automatically infers shape dimensions when needed.

### How It Works – Algorithm Breakdown
1. Pattern Parsing
The input pattern 'a b -> b a' is parsed into:

input_tokens: tokens from the left-hand side (e.g., ['a', 'b'])

output_tokens: tokens from the right-hand side (e.g., ['b', 'a'])

Parentheses like (h w) are preserved for now to represent merged axes.

2. Ellipsis Resolution
If ... is used, it is replaced by generated axis names (e.g., _axis0, _axis1) based on tensor shape and remaining known axes.

3. Shape Inference
If merged axes like (h w) appear, the function:

Computes the product of known dimensions.

Infers missing ones (e.g., infer w from h=3, shape=12 → w=4).

Ensures dimensionality matches the tensor's actual shape.

4. Input Expansion
Expands composite patterns like (h w) into flat tokens (['h', 'w']) to align with the raw tensor shape.

5. Transpose Mapping
Determines how axes should be reordered:

Builds a mapping of input axes → indices.

Constructs a transpose_order for reordering only axes that exist in both input and output.

6. Final Shape Calculation
Uses the tokenized output pattern to:

Multiply dimensions if necessary (e.g., (h w) becomes h*w)

Broadcast or reshape to desired layout.

### Performance & Efficiency
Compact & Readable: ~100 lines of clear Python, yet expressive and extensible.

Efficient: Avoids unnecessary reshapes/transposes. Only modifies axes as needed.

Broadcast-Friendly: Smart handling of broadcast scenarios like 1 -> b.

Large Dataset Ready: Designed with batching in mind. Easily integrates with tensors of shape (B, C, H, W) or larger.

Zero Dependencies: Pure NumPy implementation – fast, portable, and minimal.

### Example Usages

x = np.random.rand(2, 3, 4)

rearranged = rearrange(x, 'b c h -> b h c')  # simple transpose


x = np.random.rand(12, 10)

rearranged = rearrange(x, '(h w) c -> h w c', h=3)  # split flat spatial dim


x = np.random.rand(2, 1, 5)

rearranged = rearrange(x, 'a 1 c -> a b c', b=4)  # broadcast singleton dim


x = np.random.rand(2, 3, 4, 5)

rearranged = rearrange(x, 'b c h w -> b (c h w)')  # flatten all but batch
### Error Handling
Throws descriptive ValueError when:

Axis lengths are inconsistent or ambiguous.

Cannot reshape due to size mismatch.

Multiple unknown dimensions in merged patterns.

### Limitations
Does not yet support advanced slicing or stride tricks (like ::2 in einops).

All axes must be explicitly handled in pattern.

### Ideal Use Cases
Preprocessing images/videos for ML pipelines.

Reshaping batch data in deep learning models.

Compact manipulation of large tensors.

Readable replacements for chains of .reshape(), .transpose(), .expand_dims(), etc.

### Tests You Should Try

#### Simple spatial rearrange
x = np.random.rand(12, 10)

assert np.allclose(rearrange(x, '(h w) c -> h w c', h=3), einops.rearrange(x, '(h w) c -> h w c', h=3))

#### Broadcasting
x = np.random.rand(2, 1, 5)

assert np.allclose(rearrange(x, 'a 1 c -> a b c', b=4), einops.rearrange(x, 'a 1 c -> a b c', b=4))
"""

import numpy as np
import re
from typing import List, Tuple, Dict


def parse_pattern(pattern: str) -> Tuple[str, str]:
    if "->" not in pattern:
        raise ValueError(f"Invalid pattern '{pattern}'. Expected '->' to separate input and output.")
    return tuple(map(str.strip, pattern.split("->")))


def tokenize_pattern(pattern: str) -> List[str]:
    tokens = []
    buffer = ""
    in_paren = 0
    for ch in pattern:
        if ch == "(":
            in_paren += 1
            buffer += ch
        elif ch == ")":
            in_paren -= 1
            buffer += ch
        elif ch == " " and in_paren == 0:
            if buffer:
                tokens.append(buffer)
                buffer = ""
        else:
            buffer += ch
    if buffer:
        tokens.append(buffer)
    return tokens


def expand_pattern(tokens: List[str]) -> List[str]:
    expanded = []
    for token in tokens:
        if token == "...":
            expanded.append(token)
        elif token.startswith("("):
            sub_axes = re.findall(r'\w+', token)
            expanded.extend(sub_axes)
        else:
            expanded.append(token)
    return expanded


def resolve_ellipsis(pattern_tokens: List[str], tensor_shape: Tuple[int], other_pattern_tokens: List[str]) -> List[str]:
    # Replace ... with appropriate number of unnamed axes
    if "..." not in pattern_tokens:
        return pattern_tokens

    ellipsis_index = pattern_tokens.index("...")
    known = [t for t in pattern_tokens if t != "..."]
    num_missing = len(tensor_shape) - len(expand_pattern(known))

    if num_missing < 0:
        raise ValueError("Pattern has more named dimensions than tensor has axes.")

    unnamed_axes = [f"_axis{i}" for i in range(num_missing)]
    return pattern_tokens[:ellipsis_index] + unnamed_axes + pattern_tokens[ellipsis_index + 1:]


def extract_shape_mapping(input_tokens: List[str], shape: Tuple[int], axes_lengths: Dict[str, int]) -> Dict[str, int]:
    if len(input_tokens) != len(shape):
        raise ValueError(f"Input pattern tokens {input_tokens} don't match tensor shape {shape}")

    mapping = dict(axes_lengths)
    for token, dim in zip(input_tokens, shape):
        if token.startswith("("):  # Merged axes like (h w)
            sub_axes = re.findall(r'\w+', token)
            known = [a for a in sub_axes if a in mapping]
            unknown = [a for a in sub_axes if a not in mapping]

            if not unknown:
                # All dimensions are known, verify match
                total = np.prod([mapping[a] for a in sub_axes])
                if total != dim:
                    raise ValueError(f"Merged dimension {token} expected {total}, got {dim}")
            elif len(unknown) == 1:
                # One unknown, infer it
                known_product = np.prod([mapping[a] for a in known])
                if dim % known_product != 0:
                    raise ValueError(f"Cannot infer dimension for {unknown[0]}: {dim} not divisible by {known_product}")
                mapping[unknown[0]] = dim // known_product
            else:
                raise ValueError(f"Cannot infer more than one axis in merged dimension: {token}")
        else:
            if token in mapping and mapping[token] != dim:
                print(f"[Note] Overriding axes_lengths[{token}] = {mapping[token]} with actual shape {dim}")
            mapping[token] = dim
    return mapping


def compute_shape(tokens: List[str], axes_lengths: Dict[str, int]) -> List[int]:
    shape = []
    for token in tokens:
        if token.startswith("("):  # Merged axes
            sub_axes = re.findall(r'\w+', token)
            size = np.prod([axes_lengths[a] for a in sub_axes])
            shape.append(size)
        else:
          if token.isdigit():  # literal dimension
              shape.append(int(token))
          else:
              shape.append(axes_lengths[token])
    return shape

def compute_shape(tokens: List[str], axes_lengths: Dict[str, int]) -> List[int]:
    shape = []
    for token in tokens:
        if token.startswith("("):  # Merged axes
            sub_axes = re.findall(r'\w+', token)
            size = np.prod([axes_lengths[a] for a in sub_axes])
            shape.append(size)
        elif token.isdigit():
            shape.append(int(token))
        elif token in axes_lengths:
            shape.append(axes_lengths[token])
        else:
            raise ValueError(f"Cannot determine shape for axis '{token}'")
    return shape

def rearrange(tensor: np.ndarray, pattern: str, **axes_lengths) -> np.ndarray:
    input_pat, output_pat = parse_pattern(pattern)
    input_tokens = tokenize_pattern(input_pat)
    output_tokens = tokenize_pattern(output_pat)

    input_tokens = resolve_ellipsis(input_tokens, tensor.shape, output_tokens)
    output_tokens = resolve_ellipsis(output_tokens, tensor.shape, input_tokens)

    axes_lengths = extract_shape_mapping(input_tokens, tensor.shape, axes_lengths)

    input_expanded = expand_pattern(input_tokens)
    output_expanded = expand_pattern(output_tokens)

    # Reshape to match flat input pattern
    input_shape = [axes_lengths[t] if not t.isdigit() else int(t) for t in input_expanded]
    tensor = tensor.reshape(input_shape)

    # Transpose only input axes that appear in output, in output order
    index_map = {name: i for i, name in enumerate(input_expanded)}
    transpose_order = [index_map[name] for name in output_expanded if name in index_map]
    if transpose_order != list(range(len(transpose_order))):
        tensor = tensor.transpose(transpose_order)

    # Compute final shape and reshape or broadcast
    final_shape = compute_shape(output_tokens, axes_lengths)
    if tensor.size == np.prod(final_shape):
        tensor = tensor.reshape(final_shape)
    else:
        try:
            tensor = np.broadcast_to(tensor, final_shape)
        except Exception as e:
            raise ValueError(f"Cannot reshape or broadcast to shape {final_shape}: {e}")

    return tensor

def parse_slice_str(s: str) -> Tuple[int, int, int]:
    """Parse a slice string like '1:5:2' or '::2'."""
    parts = s.split(":")
    parts = [int(p) if p else None for p in parts]
    return slice(*parts)


def rearrange(tensor: np.ndarray, pattern: str, **axes_lengths) -> np.ndarray:
    input_pat, output_pat = parse_pattern(pattern)
    input_tokens = tokenize_pattern(input_pat)
    output_tokens = tokenize_pattern(output_pat)

    # Ellipsis
    input_tokens = resolve_ellipsis(input_tokens, tensor.shape, output_tokens)
    output_tokens = resolve_ellipsis(output_tokens, tensor.shape, input_tokens)

    # Mapping
    axes_lengths = extract_shape_mapping(input_tokens, tensor.shape, axes_lengths)

    # Expand
    input_expanded = expand_pattern(input_tokens)
    output_expanded = expand_pattern(output_tokens)

    # Reshape to flat input
    input_shape = [int(t) if t.isdigit() else axes_lengths[t] for t in input_expanded]
    tensor = tensor.reshape(input_shape)

    # Transpose if needed
    index_map = {name: i for i, name in enumerate(input_expanded)}
    transpose_order = []
    for name in output_expanded:
        if name in index_map:
            transpose_order.append(index_map[name])

    if len(transpose_order) != tensor.ndim:
        # Don't transpose if dims don't align; wait for reshape
        pass
    elif transpose_order != list(range(len(transpose_order))):
        tensor = tensor.transpose(transpose_order)


    # Final reshape
    final_shape = compute_shape(output_tokens, axes_lengths)
    if tensor.size != np.prod(final_shape):
        try:
            tensor = np.broadcast_to(tensor, final_shape)
        except Exception as e:
            raise ValueError(f"Cannot reshape or broadcast to {final_shape}: {e}")
    else:
        tensor = tensor.reshape(final_shape)

    return tensor

import numpy as np
import einops

x = np.random.rand(3, 4)
print(np.allclose(rearrange(x, 'h w -> w h'), einops.rearrange(x, 'h w -> w h')))

x = np.random.rand(12, 10)
print(np.allclose(rearrange(x, '(h w) c -> h w c', h=3), einops.rearrange(x, '(h w) c -> h w c', h=3)))

x = np.random.rand(3, 4, 5)
print(np.allclose(rearrange(x, 'a b c -> (a b) c'), einops.rearrange(x, 'a b c -> (a b) c')))

x = np.random.rand(3, 1, 5)
try:
  print(np.allclose(rearrange(x, 'a 1 c -> a b c', b=4), einops.rearrange(x, 'a 1 c -> a b c', b=4)))
except Exception as e:
  print(e)

x = np.random.rand(2, 3, 4, 5)
print(np.allclose(rearrange(x, '... h w -> ... (h w)'), einops.rearrange(x, '... h w -> ... (h w)')))

x = np.random.rand(2, 3, 4)
print(np.allclose(rearrange(x, 'b c d -> b (c d)'), einops.rearrange(x, 'b c d -> b (c d)')))

x = np.random.rand(2, 12)
print(np.allclose(rearrange(x, 'b (c d) -> b c d', c=3, d=4), einops.rearrange(x, 'b (c d) -> b c d', c=3, d=4)))

x = np.random.rand(2, 3, 4)
print(np.allclose(rearrange(x, 'b c d -> d b c'), einops.rearrange(x, 'b c d -> d b c')))

x = np.random.rand(10, 20)
print(np.allclose(rearrange(x, 'h w -> h w 1'), einops.rearrange(x, 'h w -> h w 1')))

x = np.random.rand(2, 3, 4, 5)
print(np.allclose(rearrange(x, 'b c d e -> b (c d e)'), einops.rearrange(x, 'b c d e -> b (c d e)')))

x = np.random.rand(2, 3, 4)
print(np.allclose(rearrange(x, '... d -> d ...'), einops.rearrange(x, '... d -> d ...')))

x = np.random.rand(12, 10)
print(np.allclose(rearrange(x, '(h w) c -> h w c', h=3), einops.rearrange(x, '(h w) c -> h w c', h=3)))

